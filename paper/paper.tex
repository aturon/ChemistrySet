\documentclass[preprint]{sigplanconf}

\input{packages}
\input{macros}

\begin{document}

\authorinfo{}{}{}
% yikes!
%\title{Reagents: expressing and composing patterns of fine-grained concurrency}
\title{Reagents: expressing and composing fine-grained concurrency}
\maketitle

\begin{abstract}
blah blah\\
blah blah\\
blah blah\\
blah blah\\
blah blah\\
blah blah
\end{abstract}

\section{Introduction}

\hspace{\stretch{1}}\emph{Programs are what happens between cache
  misses}.\footnote{Folklore; origin unknown.}

\subsection*{The problem}

Ahmdahl's law tells us that sequential bottlenecks fundamentally limit our
profit from parallelism~\cite{?}.  In practice, the effect is amplified by
another factor: interprocessor communication, often in the form of cache
coherence.  When one thread waits on another, the program pays the cost of
lost parallelism \emph{and} an extra cache miss.  The extra misses can easily
accumulate to yield parallel \emph{slowdown}, more than negating the benefits
of the remaining parallelism.

Cache, as ever, is king.

The easy answer is: avoid communication.  In other words, parallelize at a
coarse grain, giving threads large chunks of independent work.  But some work
doesn't easily factor into large chunks, or equal-size chunks.  Fine-grained
parallelism is easier to find, and easier to sprinkle throughout existing
sequential code.
%With increasing pressure to parallelize, fine-grained parallelism will only
%become more common.

Another answer is: communicate efficiently.  The past two
decades\footnote{Herlihy/Wing, Mellor-Crummey/Scott} have produced a sizable
collection of algorithms for synchronization, communication, and shared
storage which minimize the use of memory bandwidth and avoid unnecessary
waiting.  This research effort has led to industrial-strength
libraries---\texttt{java.util.concurrent} (JUC) the most prominent---offering
a wide range of concurrency primitives appropriate for fine-grained
parallelism.

Such libraries are an enormous undertaking---and one that must be repeated for
new platforms.  They tend to be conservative, implementing only those data
structures and primitives likely to fulfill common needs.  In addition, it is
generally not possible to safely combine the facilities of the library.  For
example, JUC provides queues, sets and maps, but not stacks or bags.  Its
queues come in both blocking and nonblocking forms, while its sets and maps
are nonblocking only.  Although the queues provide atomic (thread-safe)
dequeuing and sets provide atomic insertion, it is not possible to combine
these into a single atomic operation that moves an element from a queue into a
set.

In short, libraries for fine-grained concurrency are indispensable, but hard
to write, hard to extend by composition, and hard to tailor to the needs of
particular users.

\subsection*{Our contribution}

% first class synchronous operations

%% We have developed a small set of primitives for fine-grained concurrency that
%% are \emph{expressive} and \emph{composable}:

%% We have developed \emph{reagents}, a new abstraction for fine-grained
%% concurrency that is \emph{expressive} and \emph{composable}:

%% We have developed \emph{reagents}, which are first-class representations of
%% fine-grained concurrency primitives.  Reagents are \emph{expressive} and
%% \emph{composable}:

We have developed \emph{reagents}, which abstractly represent fine-grained
concurrent operations.  Reagents are \emph{expressive} and \emph{composable},
but when invoked retain the scalability and performance of existing
algorithms:

%We have developed \emph{reagents}, 

%% compositional primitives for fine-grained concurrency

%% a small set of primitives for fine-grained concurrency that
%% are \emph{expressive} and \emph{composable}:

\paragraph{Expressive}  .

\paragraph{Composable} .

We see this work as serving the needs of two distinct groups: concurrency
experts and concurrency users.  Using reagents, experts can write algorithms
more easily, because common patterns are expressible as abstractions and many
are built-in.  Users can compose, extend, and tailor the library without
detailed knowledge of the algorithms involved.

blend shared-state and message-passing concurrency

implemented a Scala library

\elide{
# Contributions

  - make it easier to express fine-grained concurrent algorithms,
  including blocking operations etc.  make it possible to build them
  compositionally, which already happens in the literature but is
  largely unremarked upon.

  - we capture exponential backoff, elimination backoff, and other
  such patterns once and for all

  - can use flat combining as illustration of capturing very general
  concurrency abstraction

  - smooth blend of message-passing and shared-state concurrency
  isolation for shared state, coherent communication through messages,
  uniform blocking/synchronization

  - operational semantics

  - yields completely lock-free implementation of core CML

  - generalizes join calculus implementation, allowing dissolution
  even for existing channels

  - generalizes transactional events, but with more specialized
  implementation (doesn't need an underlying STM)

by no means a silver bullet.

why is this different from STM
}

\section{Background}

Broadly, we are interested in data structures and algorithms for
communication, synchronization, or both.  This section gives a brief
survey of the most important techniques for communication and
synchronization in a fine-grained setting---exactly the techniques
that reagents abstract and generalize.  Readers already familiar with
Treiber stacks~\cite{?}, elimination-backoff stacks~\cite{?}, dual
stacks~\cite{?}, and MCS locks~\cite{?} can safely skip to
\secref{reagents}.

Given our target of cache-coherent, shared-memory architectures, the
most direct way of communicating between threads is modifying shared
memory.  The challenge is to provide both \emph{atomicity} and
\emph{scalability}: communications must happen concurrently both
without corruption and without clogging the limited memory bandwidth,
even when many cores are communicating.  A simple way to provide
atomicity is to associate a lock with each shared data structure,
acquiring the lock before performing any operation.  The prevailing
wisdom\footnote{With some notable exceptions, like the recent work on
  \emph{flat combining}~\cite{?}, which we explore in \secref{?}.} is
that such \emph{coarse-grained} locking is inherently unscalable:
\begin{itemize}
\item It forces operations on the data structure to be serialized,
  even when they could be performed in parallel.
\item It adds extra cache-coherence traffic, since each core must
  acquire the same lock's cache line in exclusive mode before
  operating.  For fine-grained communication, that means at least one
  cache miss per operation.
\item It is susceptible to ... % blocking %TODO
\end{itemize}

To combat scalability problems, data structures employ finer-grained locking,
or eschew locking altogether.  Fine-grained locking associates locks with
small, independent parts of a data structure, allowing those parts to be
manipulated in parallel.  Lockless (or \emph{nonblocking}) data structures
instead perform updates directly, using hardware-level operations (like
compare-and-set) to ensure atomicity.  Doing the updates directly means that
there is no extra communication or contention for locks, but it also generally
means that the entire update must consist of changing a single word of
memory---a constraint which is often quite challenging to meet.  

\begin{figure}
\begin{lstlisting}
class TreiberStack[A] {
  private val head = new AtomicReference[List[A]](Nil)
  def push(a: A) {
    val backoff = new Backoff
    while (true) {
      val cur = head.get()
      if (head.cas(cur, a :: cur)) return
      backoff.once()
    }
  }
  def tryPop(): Option[A] = {
    val backoff = new Backoff
    while (true) {
      val cur = head.get() 
      cur match {
        case Nil     => return None
        case a::tail => 
          if (head.cas(cur, tail)) return Some(a)
      }
      backoff.once()
    }
  }
}
\end{lstlisting}
\nocaptionrule
\caption{Treiber's stack (in Scala)}
\label{fig:classic-treiber}
\end{figure}

\figref{classic-treiber} gives a classic example of a fine-grained
concurrent data structure: Treiber's lock-free stack~\cite{?}.  The
stack is represented as an \emph{immutable} linked list.  Mutation
occurs solely through the \lstinline{head} pointer, represented here
as an \lstinline{AtomicReference}.  The \lstinline{head} is mutated
using \lstinline{compareAndSet} (here abbreviated as \lstinline{cas}),
which takes an expected value and a new value, and atomically updates
the reference

%notice: changed to less desirable representation to make CAS work

%Most of the Scala code in this paper should be 
%need to explain pattern matching and PartialFunction

%% Say something about CONTENTION.

Treiber's stack scales better than a lock-based stack mainly because
it decreases the amount of shared data that must be updated per
operation: instead of acquiring a shared lock, altering a shared stack
pointer, and releasing the shared lock, Treiber's stack does a single
CAS to update the shared pointer directly.  But the head pointer of
the stack is still a centralized source of contention.  Spreading out
the contention requires parallelizing stack operations---but how,
given that they all involve modifying the top of the stack?

Parallelization requires a change of perspective.  Normally, we view
concurrent operations on the same part of a data structure as
competing to atomically update that data structure; we want the
operations to be isolated.  But if 

%talk about elimination backoff and blocking versions but don't show code.

discuss the insight behind MCS locks

\section{Reagents}
\label{sec:reagents}

In the last section, we saw a range of strategies for communication
and synchronizion at a fine grain:
\begin{itemize}
\item using CAS to make direct, atomic updates;
\item using exponential backoff to ease contention; 
\item using side-channel communication to increase parallelism; 
\item using disjoint spinwaiting to eliminate useless cache traffic; 
\item using dual data structures to incorporate blocking.
\end{itemize}
Reagents incorporate these disparate techniques into a uniform framework for
writing concurrent algorithms.  Some of the techniques are built into the
framework, while others are expressible as reusable abstractions.

more easily and concisely
% extensibility and composability

\subsection{The main idea}

Reagents are a new instance of an old idea: computations can be usefully
represented as data.

% no reason not to present this as an ADT


%% The aim of reagents is to make it easy to express these and other
%% patterns, while providing compositionality blah blah

%% First we give an overview of the library, and then we illustrate its
%% expressiveness and composability by showing how to implement several 
 
chemistry metaphor

lack of isolation needed to express many lock-free algorithms.
message-passing turns out to be a good way to understand what's going on.

\subsection{Expressiveness by example}

\begin{lstlisting}
class TreiberStack[A] {
  private val head = new Ref[List[A]](Nil)
  val push: Reagent[A, Unit] = head.upd {
    (xs, x) => (x:xs, ())
  }
  val tryPop: Reagent[Unit, Option[A]] = head.upd {
    case (x::xs, ()) => (xs, Some(x))
    case (Nil,   ()) => (Nil, None)
  }
}
\end{lstlisting}

Examples:

\begin{itemize}
\item TreiberStack
\item with blocking
\item with elimination backoff
\item MSQueue
\item with blocking
\item with buckets?
\item Counter to semaphore
\item RWLocks
\item Tree barriers
\item Dining philosophers(ish)
\item Flat combining - doesn't work if commit is two-phase process.
  probably too much to bring into scope for this paper, anyway
\end{itemize}

\begin{lstlisting}
class MSQueue[A] {
  private case class Node(data: A, next: Ref[Node])
  private val head = Ref(Node(null))
  private val tail = Ref(head.read!())

  @tailrec private def search(x:A): Reagent[Unit,Unit] = 
    tail.read ! () match {
      case Node(_, r@Ref(null)) => 
        r.cas(null, Node(x, new Ref(null)))
      case ov@Node(_, Ref(nv)) => 
        tail.cas(ov,nv) !? (); search(x)
    }
  val enq: Reagent[A, Unit] = computed search
  val tryDeq: Reagent[Unit, Option[A]] = head.upd {
    case Node(_, Ref(n@Node(x, _))) => (n, Some(x))
    case emp => (emp, None)
  }
}
\end{lstlisting}

\section{Semantics}
\label{sec:semantics}

Try to keep this minimal for the sake of space

Purpose of the semantics is mainly to clarify the relationship between
the shared-state and message-passing aspects of reagents.

(fractional?) permissions model

\begin{itemize}
  \item read, cas
  \item send
  \item choice
  \item react
  \item bind and ret
  \item computed?
\end{itemize}

\elide{
\[
\infer
  {P_i, \sigma_i \cstep{t_i} P'_i, \sigma'_i}
  {P_1 | \cdots | P_n,\ \sigma_1 \uplus \cdots \uplus \sigma_n\ \step\
   P'_1 | \cdots | P'_n,\ \sigma'_1 \uplus \cdots \uplus \sigma'_n
  }
\]

\[
\infer
  {r_i!v_i, \sigma_i \cstep{t_i} \ret{v'_i}, \sigma'_i}
  {\procCtx[\prod \evalCtx_i[r_i ! v_i]],\ \biguplus \sigma_i \step
   \procCtx[\prod \evalCtx_i[\ret{v'_i}]],\ \biguplus \sigma'_i}
\]
}

\[
\infer
  {r_i!v_i, \sigma_i \cstep{t_i} \ret{v'_i}, \sigma'_i}
  {\prod \evalCtx_i[r_i ! v_i],\ \biguplus \sigma_i \step
   \prod \evalCtx_i[\ret{v'_i}],\ \biguplus \sigma'_i}
\]

\section{Implementation}
\label{sec:implementation}

\begin{lstlisting}
def tryReact(a: A, rx: Reaction, offer: Offer[B]): Any = 
  r1.tryReact(a, rx, offer) match {
    case Retry => 
      r2.tryReact(a, rx, offer) match {
        case Retry => Retry
        case Block => Retry // must retry r1
        case ans   => ans
      }
    case Block => r2.tryReact(a, rx, offer)
    case ans => ans
  }
\end{lstlisting}

\section{Performance}
\label{sec:performance}



\section{Related work}
\label{sec:related}

We believe it is crucial to have XX, YY together

STM, Communicating transactions and company, elastic transactions and company

CML, Transactional events, join calculus

Paper on composing lock-free data structure operations

\bibliographystyle{abbrvnat}
\bibliography{reagents}

\end{document}

# Intro notes

What do we want to accomplish?

 - motivate libraries like j.u.c
 - downsides of such libraries
   - huge research & impl effort
   - not easily extensible
   - internal structure has repeated, but unabstracted patterns
 - our contribution
   - a new way of expressing scalable algorithms that
     - doesn't lose much performance 
     - allows composition
     - is much nicer than direct programming
     - also incorporates blocking behavior

# General notes

Imposes no cache-coherence overhead on isolated data structures

Can argue that transactional-event-style liveness properties are
important if you want general way of composing fine-grained concurrent
algorithms with message-passing

To what extent do reagents encompass STM?  who are we competing with?

multiparadim, expressive, fast -- all that's great, but sales pitch
needs to be oriented around a customer with specific needs.  any
reason not to use the pitch of extensible, composable j.u.c.?

# To do

  - benchmark against STM (possibly Akka Transactors?)

  - benchmark queues, including flat combining and buckets

  - somewhere say that: we examine fairly simple examples in the
  paper, due to size constraints.  but we can handle more complex
  cases like sets/maps represented via skip lists, basket queues, 
  and are exploring flat combining.

# Outline

1.2 Title, abstract, intro, 
1.0 Background
2.5 Library overview & examples
1.5 Semantics 
1.5 Implementation/algorithms 
0.5 Performance results
1.0 Related work
0.8 Bib

How much time should we spend covering scalable concurrency
background?  This is perhaps tied to strategies for introducing the
library constructs: the primitives can be shown by-need for examples.
Should consult good papers like Ryan's and Jesse's for examples here.

# Semantics

Need to nail down story for overlapping state access.  In particular,
when (if ever) is an error flagged?  Unfortunately, talking about heap
unsplittability is rather more complicated than splittability.
