\documentclass[preprint]{sigplanconf}

\input{packages}
\input{macros}

\begin{document}

\authorinfo{}{}{}
% yikes!
%\title{Reagents: expressing and composing patterns of fine-grained concurrency}
\title{Reagents: expressing and composing fine-grained concurrency}
\maketitle

\begin{abstract}
blah blah\\
blah blah\\
blah blah\\
blah blah\\
blah blah\\
blah blah
\end{abstract}

\section{Introduction}

\hspace{\stretch{1}}\emph{Programs are what happens between cache
  misses}.\footnote{Folklore; origin unknown.}

\subsection*{The problem}

Ahmdahl's law tells us that sequential bottlenecks fundamentally limit our
profit from parallelism~\cite{?}.  In practice, the effect is amplified by
another factor: interprocessor communication, often in the form of cache
coherence.  When one thread waits on another, the program pays the cost of
lost parallelism \emph{and} an extra cache miss.  The extra misses can easily
accumulate to yield parallel \emph{slowdown}, more than negating the benefits
of the remaining parallelism.

Cache, as ever, is king.

The easy answer is: avoid communication.  In other words, parallelize at a
coarse grain, giving threads large chunks of independent work.  But some work
doesn't easily factor into large chunks, or equal-size chunks.  Fine-grained
parallelism is easier to find, and easier to sprinkle throughout existing
sequential code.
%With increasing pressure to parallelize, fine-grained parallelism will only
%become more common.

Another answer is: communicate efficiently.  The past two
decades\footnote{Herlihy/Wing, Mellor-Crummey/Scott} have produced a sizable
collection of algorithms for synchronization, communication, and shared
storage which minimize the use of memory bandwidth and avoid unnecessary
waiting.  This research effort has led to industrial-strength
libraries---\texttt{java.util.concurrent} (JUC) the most prominent---offering
a wide range of concurrency primitives appropriate for fine-grained
parallelism.

Such libraries are an enormous undertaking---and one that must be repeated for
new platforms.  They tend to be conservative, implementing only those data
structures and primitives likely to fulfill common needs.  In addition, it is
generally not possible to safely combine the facilities of the library.  For
example, JUC provides queues, sets and maps, but not stacks or bags.  Its
queues come in both blocking and nonblocking forms, while its sets and maps
are nonblocking only.  Although the queues provide atomic (thread-safe)
dequeuing and sets provide atomic insertion, it is not possible to combine
these into a single atomic operation that moves an element from a queue into a
set.

In short, libraries for fine-grained concurrency are indispensable, but hard
to write, hard to extend by composition, and hard to tailor to the needs of
particular users.

\subsection*{Our contribution}

% first class synchronous operations

%% We have developed a small set of primitives for fine-grained concurrency that
%% are \emph{expressive} and \emph{composable}:

%% We have developed \emph{reagents}, a new abstraction for fine-grained
%% concurrency that is \emph{expressive} and \emph{composable}:

%% We have developed \emph{reagents}, which are first-class representations of
%% fine-grained concurrency primitives.  Reagents are \emph{expressive} and
%% \emph{composable}:

We have developed \emph{reagents}, which abstractly represent fine-grained
concurrent operations.  Reagents are \emph{expressive} and \emph{composable},
but when invoked retain the scalability and performance of existing
algorithms:

%We have developed \emph{reagents}, 

%% compositional primitives for fine-grained concurrency

%% a small set of primitives for fine-grained concurrency that
%% are \emph{expressive} and \emph{composable}:

\paragraph{Expressive}  .

\paragraph{Composable} .

We see this work as serving the needs of two distinct groups: concurrency
experts and concurrency users.  Using reagents, experts can write algorithms
more easily, because common patterns are expressible as abstractions and many
are built-in.  Users can compose, extend, and tailor the library without
detailed knowledge of the algorithms involved.

blend shared-state and message-passing concurrency

implemented a Scala library

CLEAR COST MODEL

\elide{
# Contributions

  - make it easier to express fine-grained concurrent algorithms,
  including blocking operations etc.  make it possible to build them
  compositionally, which already happens in the literature but is
  largely unremarked upon.

  - we capture exponential backoff, elimination backoff, and other
  such patterns once and for all

  - can use flat combining as illustration of capturing very general
  concurrency abstraction

  - smooth blend of message-passing and shared-state concurrency
  isolation for shared state, coherent communication through messages,
  uniform blocking/synchronization

  - operational semantics

  - yields completely lock-free implementation of core CML

  - generalizes join calculus implementation, allowing dissolution
  even for existing channels

  - generalizes transactional events, but with more specialized
  implementation (doesn't need an underlying STM)

by no means a silver bullet.

why is this different from STM
}

\section{Background}
\label{sec:background}

Broadly, we are interested in data structures and algorithms for
communication, synchronization, or both.  This section gives a brief
survey of the most important techniques for communication and
synchronization in a fine-grained setting---exactly the techniques
that reagents abstract and generalize.  Readers already familiar with
Treiber stacks~\cite{?}, elimination-backoff stacks~\cite{?}, dual
stacks~\cite{?}, and MCS locks~\cite{?} can safely skip to
\secref{reagents}.

Given our target of cache-coherent, shared-memory architectures, the
most direct way of communicating between threads is modifying shared
memory.  The challenge is to provide both \emph{atomicity} and
\emph{scalability}: communications must happen concurrently both
without corruption and without clogging the limited memory bandwidth,
even when many cores are communicating.  A simple way to provide
atomicity is to associate a lock with each shared data structure,
acquiring the lock before performing any operation.  The prevailing
wisdom\footnote{With some notable exceptions, like the recent work on
  \emph{flat combining}~\cite{?}, which we explore in \secref{?}.} is
that such \emph{coarse-grained} locking is inherently unscalable:
\begin{itemize}
\item It forces operations on the data structure to be serialized,
  even when they could be performed in parallel.
\item It adds extra cache-coherence traffic, since each core must
  acquire the same lock's cache line in exclusive mode before
  operating.  For fine-grained communication, that means at least one
  cache miss per operation.
\item It is susceptible to preemptions or stalls of a
  thread holding a lock, which prevents other threads from making progress.
\end{itemize}

To achieve scalability, data structures employ finer-grained locking,
or eschew locking altogether.  Fine-grained locking associates locks
with small, independent parts of a data structure, allowing those
parts to be manipulated in parallel.  Lockless (or \emph{nonblocking})
data structures instead perform updates directly, using hardware-level
operations (like compare-and-set) to ensure atomicity.  Doing the
updates directly means that there is no extra communication or
contention for locks, but it also generally means that the entire
update must consist of changing a single word of memory---a constraint
which is often quite challenging to meet.

\begin{figure}
\begin{lstlisting}
class TreiberStack[A] {
  private val head = new AtomicReference[List[A]](Nil)
  def push(a: A) {
    val backoff = new Backoff
    while (true) {
      val cur = head.get()
      if (head.cas(cur, a :: cur)) return
      backoff.once()
    }
  }
  def tryPop(): Option[A] = {
    val backoff = new Backoff
    while (true) {
      val cur = head.get() 
      cur match {
        case Nil     => return None
        case a::tail => 
          if (head.cas(cur, tail)) return Some(a)
      }
      backoff.once()
    }
  }
}
\end{lstlisting}
\nocaptionrule
\caption{Treiber's stack (in Scala)}
\label{fig:classic-treiber}
\end{figure}

\figref{classic-treiber} gives a classic example of a fine-grained concurrent
data structure: Treiber's lock-free stack~\cite{?}.  The stack is represented as
an \emph{immutable} linked list.  Mutation occurs solely through the
\lstinline{head} pointer, represented here as an \lstinline{AtomicReference}.
The \lstinline{head} is mutated using \lstinline{compareAndSet} (here
abbreviated as \lstinline{cas}), which takes an expected value and a new value,
and atomically updates the reference if it has the expected value.  It returns
\lstinline{true} iff the update was successful.

A word about Scala: the syntax 
\begin{lstlisting}
  exp match { case pat1 => body1 ... case patN => bodyN }
\end{lstlisting}
denotes pattern matching, where the value of \lstinline{exp} is matched in order
against \lstinline{pat1} up to \lstinline{patN}.  In \figref{classic-treiber},
we use the two list constructors \lstinline{Nil} and \lstinline{::} (an inline
cons) in patterns.

The \lstinline{push} and \lstinline{tryPop} operations are implemented in a
typical \emph{optimistic} style: they take a snapshot of the head, perform some
local computation with it, and then attempt to update it accordingly.  The
computation is optimistic because it is performed without holding a lock.  The
head might concurrently change, invalidating the \lstinline{cas} intended to
update it.  To account for possible interference, the snapshot-compute-update
code executes within a retry loop.  While the loop may retry forever, it can
only do so by repeatedly failing to \lstinline{cas}, which in turn means that an
unlimited number of other operations on the stack are succeeding.  In other
words, \lstinline{push} and \lstinline{tryPop} are formally \emph{lock
  free}~\cite{?}.

To successfully \lstinline{cas}, a core must acquire the relevant cache line in
\emph{exclusive} mode~\cite{?}, or something conceptually equivalent.  When
several cores attempt to \lstinline{cas} a common reference, they must
coordinate at the hardware level, using precious memory bandwidth and wasting
processor cycles.  A failed \lstinline{cas} is evidence that there is
\emph{contention} over a common location.  To increase the \lstinline{cas}
success rate and conserve memory bandwidth, fine-grained concurrent algorithms
employ a \emph{backoff} scheme, which here we have abstracted into a lightweight
\lstinline{Backoff} class.  The class encapsulates the simplest scheme:
busy-wait for a random amount of time that grows exponentially each time
\lstinline{once} is invoked.  

%notice: changed to less desirable representation to make CAS work

%Most of the Scala code in this paper should be 
%need to explain pattern matching and PartialFunction

Treiber's stack scales better than a lock-based stack mainly because it
decreases the amount of shared data that must be updated per operation: instead
of acquiring a shared lock, altering a shared stack pointer, and releasing the
shared lock, Treiber's stack does a single CAS to update the shared pointer
directly.  However, that pointer is still a centralized source of contention.
While exponential backoff helps relieve the contention, we can do better by
parallelizing stack operations---which is counterintuitive, given that they all
involve modifying the top of the stack.

Parallelization requires a change of perspective.  Normally, we view concurrent
operations on the same part of a data structure as competing to atomically
update that data structure; we want the operations to be isolated.  On the other
hand, sometimes operations can ``help'' each other: a \lstinline{push} and a
\lstinline{tryPop} effectively cancel each other out.  This insight leads to a
scheme called \emph{elimination backoff}~\cite{?}.  Operations first try the
usual \lstinline{cas}-based code.  If the \lstinline{cas} fails, rather than
busy-waiting, the operations advertise their presence on a side-channel,
reducing the contention for the head pointer.  If a \lstinline{push} and
\lstinline{tryPop} detect their mutual presence, the \lstinline{push} can pass
its argument directly to \lstinline{tryPop} through the side-channel, and no
change to the head pointer is necessary.  Atomicity is not violated, because had
the \lstinline{push} and \lstinline{pop} executed in sequence, the head pointer
would have been returned to its original value anyway.

Both \lstinline{push} and \lstinline{tryPop} are \emph{total} operations: they
can in principle succeed no patter what state the stack is in, and fail (and
retry) only due to active interference from concurrent threads.  A true
\lstinline{pop} operation, on the other hand, is \emph{partial}: it is undefined
when the stack is empty.  Often this is taken to mean that the operation should
block until another thread moves the stack into a state on which the operation
is defined.  Partial operations introduce considerable complexity, because
\emph{all} of the operations on the data structure must potentially signal
blocked threads, depending on the changes being performed.  In some cases, it is
possible to cleverly treat signalling in essentially the same way as atomic
updates~\cite{?}.

Synchronization and signalling is also subject to cache and memory bandwidth
concerns.  For example, when a lock is released, how much memory traffic is
generated?  If the lock is implemented as a single bit which threads attempt to
\lstinline{cas} in a tight loop, any change to that bit will result in a cascade
of memory traffic---even though only one waiting thread can actually win and
acquire the lock.  The most important strategy for cutting down this traffic was
introduced by Mellor-Crummey and Scott~\cite{?}, who suggested that waiting
threads should place themselves in a lock-free queue, and the release operation
should  % TODO: room for this??

%talk about elimination backoff and blocking versions but don't show code.

%discuss the insight behind MCS locks

The goal of reagents is to systematize these techniques and others into a
uniform set of combinators that make algorithms easier to express, and allows
algorithms to be composed and tailored by users.  In the next section, we will
see how to write all of the above examples, concisely and composably.

\elide{
In \secref{background}, we saw a range of strategies for communication and
synchronizion at a fine grain:
\begin{itemize}
\item using CAS to make direct, optimistic, and atomic updates;
\item using exponential backoff to ease contention; 
\item using side-channel communication to increase parallelism; 
\item using disjoint spinwaiting to eliminate useless cache traffic; 
\item using dual data structures to incorporate blocking.
\end{itemize}
Reagents incorporate these disparate techniques into a uniform set of
combinators for writing concurrent algorithms.  Some of the techniques are
built into the combinators themselves, while others are expressible as
reusable abstractions.
}

\section{Reagents}
\label{sec:reagents}

\begin{figure}
\begin{lstlisting}[mathescape=true,frame=single]
// combinators for shared state
upd:  Ref[A] => PartialFunction[(A,B), (A,C)] 
             => Reagent[B,C]
read: Ref[A] => Reagent[Unit, A]
cas:  Ref[A] * A * A => Reagent[Unit, Unit]

// combinators for message passing
swap: Endpoint[A,B] => Reagent[A,B]

// combinators for composition
+  : Reagent[A,B] * Reagent[A,B] => Reagent[A,B]
>> : Reagent[A,B] * Reagent[B,C] => Reagent[A,C]
|||  : Reagent[A,B] * Reagent[A,C] => Reageng[A,(B,C)]  

// combinators for computation
ret: A => Reagent[Unit, A]
computed: PartialFunction[A, Reagent[Unit, B]]
          => Reagent[A,B]
postCommit: (A => Unit) => Reagent[A,A]
\end{lstlisting}
\label{fig:combinators}
\end{figure}

Reagents are a new instance of an old idea: representing computations as data.
The computations being represented are fine-grained concurrent operations, so
a value of type \reagent{A}{B} represents a function from \lstinline{A} to
\lstinline{B} that internally interacts with a concurrent data structure
through mutation, synchronization, or both.

Unlike $\lambda$-abstractions, which encompass arbitrary code and treat that
code opaquely, reagents are built up from a limited set of \emph{combinators}
that are known to the implementation.  Existing reagents---for example, those
built by a concurrency expert---can be composed or tailored by library users
by applying additional combinators, without knowledge of the internal
implementation.  At any point a user can invoke a reagent, either as a
\emph{reactant} or a \emph{catalyst}---a distinction explained in
\secref{easy-examples}.

Reagents are patterned after Concurrent ML~\cite{?} and Haskell's STM
monad~\cite{?}, which represent (respectively) synchronous and atomic
operations as first-class values; we discuss the relationship in
\secref{related}.
%
Our contribution is giving a set of combinators appropriate for expressing and
composing fine-grained concurrent algorithms (\secref{easy-examples}) with a
clear semantics (\secref{semantics}), ... %TODO

%more easily and concisely
% extensibility and composability

%% The aim of reagents is to make it easy to express these and other
%% patterns, while providing compositionality blah blah

%% First we give an overview of the library, and then we illustrate its
%% expressiveness and composability by showing how to implement several 

\elide{
want to talk about blocking versus retryable failure
  - probably can wait until CAS introduced, toward end
  - on the other hand, need to be clear about how this works 
    for upd.  maybe introducing the distinction early on makes 
    sense.

where should we introduce react versus catalyze?
  - want to talk about react early; perhaps introduce them 
    separately?

diagram(s) of reaction?
  - we'll see if there's space...

  upd
    nonblocking: 
      Treiber
    blocking: 
      Treiber
      Counter/semaphore
  composition
    Treiber & counter?
    Treiber & lift => blocking
  messages and choice
    elimination Treiber (& counter!  implications for semaphores?)
    blocking
  catalysts
    zip and unzip, tree barriers
  computed, read, and cas - retryable failure, postCommit?
    msqueue
  parallel composition?
  timeouts?  TODO
}

\subsection{Atomic updates on \lstinline{Ref}s}

Memory is shared between reagents using the type \lstinline{Ref[A]} of
atomically-updatable references.\footnote{ These references are only
  accessible through reagent combinators, which allows them to provide
  functionality beyond the built-in \lstinline{AtomicReference[A]} type, such
  as the blocking logic we will see shortly.  }
The \lstinline{upd} combinator~(\figref{combinators}) represents simple atomic
updates to references.  It takes an \emph{update function}, which tells how to
transform a snapshot of the reference cell and some input into an updated
value for the cell and some output.  Using \lstinline{upd}, we can rewrite
\lstinline{TreiberStack} in a more readable and concise way:
\begin{lstlisting}
class TreiberStack[A] {
  private val head = new Ref[List[A]](Nil)
  val push: Reagent[A, Unit] = upd(head) {
    case (xs, x) => (x:xs, ())
  }
  val tryPop: Reagent[Unit, Option[A]] = upd(head) {
    case (x::xs, ()) => (xs, Some(x))
    case (Nil,   ()) => (Nil, None)
  }
}
\end{lstlisting}
In Scala, anonymous partial functions are written as a series of cases
enclosed in braces.  For \lstinline{push} and \lstinline{tryPop}, the case
analyses are exhaustive, so the update functions are in fact total.

Being reagents, \lstinline{push} and \lstinline{tryPop} are inert values.
They can be invoked using the \lstinline{!} method provided by reagents; for a
\lstinline{Reagent[A,B]} the \lstinline{!} method takes an \lstinline{A} and
returns a \lstinline{B}.  Scala permits infix notation for methods, so we can
use a \lstinline{TreiberStack} as follows:
\begin{lstlisting}
  var s = new TreiberStack[Int]; s.push ! 42
\end{lstlisting}
The key point is that when we invoke these reagents, we are executing
\emph{exactly} the same algorithms written by hand in \secref{background},
including the retry loop with exponential backoff; reagents systematize and
internalize common patterns of fine-grained concurrency.  But by exposing
\lstinline{push} and \lstinline{tryPop} as reagents, rather than methods, we
leave the door open for further composition and tailoring by a user of the
data structure (\secref{choice}, \secref{conjunction}).

While \lstinline{tryPop} handles both empty and nonempty stacks, we
can write a variant that drops the empty case:
\begin{lstlisting}
val pop: Reagent[Unit, A] = upd(head) {
  case (x::xs, ()) => (xs, x)
}
\end{lstlisting}
Now our update function is partial.  An invocation \lstinline{s.pop ! ()} will
block the calling thread unless or until the stack is nonempty.  Blocking and
signalling are entirely handled by the reagent implementation, using standard
techniques~(\secref{implementation}); there is therefore no risk of a lost
wakeup.  %TODO: add more here?

\elide{
% What's the cost/benefit for this example?
isomorphic to a stack of unit values
\begin{lstlisting}
class Counter {
  private val c = new Ref[Int](0)
  val inc = upd(c) { case (i, ()) => (i+1, i) }
  val dec = upd(c) { case (i, ()) if (i > 0) => (i-1, i) }
  val tryDec = upd(c) {
     case (i, ()) if (i == 0) => (0,   None)
     case (i, ())              => (i-1, Some(i))
  }
}
\end{lstlisting}

Roughly equivalent to the unfair JUC semaphore implementation.
}

%% THIS SEEMS MORE CONFUSING THAN HELPFUL
%% For now, it suffices to know that
%% invoking a \lstinline{cas} will complete and return \lstinline{()} only when
%% the CAS has successfully completed.

%% \begin{lstlisting}[mathescape=true]
%% def upd[A,B,C](r: Ref[A], f: (A,B) $\rightharpoonup$ (A,C)): Reagent[B,C] 
%%   = computed { 
%%       (b: B) =>       
%% \end{lstlisting}

\subsection{Synchronization: how reagents react}

%% References allow threads to communicate through atomic changes to
%% shared state.  The atomic updates appear to happen in a sequence
%% without overlaps: concurrent threads cannot interfere with each other
%% or otherwise detect that they are running concurrently.  

Another staple of concurrency libraries is synchronization, whereby
threads are made mutually aware of reaching a particular state.
Synchronization is complementary to shared state: the former
provides concurrent interaction, while the latter (through atomic
update) provides sequential isolation.  % TODO: make this less cryptic

Reagents interact through \emph{synchronous swap channels}, which
consist of two complementary \emph{endpoints}:
\begin{lstlisting}
  mkChan[A,B]: (Endpoint[A,B], Endpoint[B,A])
\end{lstlisting}
The combinator for communication is \lstinline{swap} (see
\figref{combinators}), which lifts an \lstinline{Endpoint[A,B]} to a
\lstinline{Reagent[A,B]}.  When two reagents communicate on opposite
endpoints, they provide messages of complementary type (\lstinline{A}
and \lstinline{B}, for example) and receive each other's value.  We
call a successful communication a \emph{reaction} between reagents.
On the other hand, if no complementary message is available,
\lstinline{swap} will block until a reaction can take place.  Once
again, the waiting and signalling protocol is managed internally by
the reagent implementation, so lost wakeups are impossible.

The \lstinline{swap} combinator alone is not powerful enough to
express many synchronization primitives.  It becomes powerful when
used together with other combinators, particularly \emph{choice} and
\emph{sequence}.

%% lack of isolation needed to express many lock-free algorithms.
%% message-passing turns out to be a good way to understand what's going on.

%% key point: traditional lock-free algorithms that use a single cas do not pay
%% any penalty for composability/kcas potential

\subsection{Composing reagents: choice}
\label{sec:choice}

If \lstinline{r} and \lstinline{s} are two reagents of the same type,
their \emph{choice} \lstinline{r + s} will behave like one of them,
nondeterministically, when invoked.  The most straightforward use of
choice is in the spirit of Unix's \lstinline{select}~\cite{?}: waiting
on several signals simultaneously, while consuming only one of them.
For example, if \lstinline{c} and \lstinline{d} are endpoints of the
same type, \lstinline{swap(c) + swap(d)} is a reagent that will accept
exactly one message, either from \lstinline{c} or from \lstinline{d}.
If neither endpoint has a message available, the reagent will block
until one of them does.  As usual, the blocking protocol is handled by
the reagent library.

A more novel use of choice is adding backoff strategies (of the kind
described in \secref{?}).  For example, we can build an elimination
backoff stack as follows:
\begin{lstlisting}
class EliminationStack[A] {
  private val s = new TreiberStack[A]
  private val (elimPop, elimPush) = mkChan[Unit,A]
  val push: Reagent[A,Unit] = s.push + swap(elimPush)
  val pop:  Reagent[Unit,A] = s.pop  + swap(elimPop)
}
\end{lstlisting}
Choice is left-biased,\footnote{The left bias makes choice more
  similar to \lstinline{orElse} in Haskell's STM than to CML's choice,
  but there are differences from both.  See \secref{related}.  } so
when \lstinline{push} is invoked, it will first attempt to push onto
the underlying Treiber stack.  If the underlying push fails (due to a
lost CAS race), \lstinline{push} will attempt to send a message along
\lstinline{elimPush}, \emph{i.e.}, to synchronize with a concurrent
popper.  If it succeeds, the \lstinline{push} reagent completes
without ever having modified its underlying stack.

We noted that \lstinline{s.push} can only fail due to a failed CAS.
Such a failure is \emph{transient}: it can only be caused by active
interference from another thread.  A reagent that has failed
transiently should retry, rather than block, following the concurrency
patterns laid out in \secref{background}.  On the other hand,
\lstinline{s.pop} can fail in a \emph{permanent} way: the underlying
stack \lstinline{s} may be empty.  This failure is permanent in the
sense that only activity by another thread can enable the reagent to
proceed.  When faced with a permanent failure, a reagent should block
until signalled that the underlying state has changed.

For choice, failure depends on the underlying reagents.  A choice
reagent fails permanently only when \emph{both} of its underlying
reagents have failed permanently.  On the other hand, if either fails
transiently, the choice reagent has failed transiently and should
therefore retry.  Reasoning along these lines, we deduce that
\lstinline{push} never blocks, since the underlying \lstinline{s.push}
can only fail transiently.  On the other hand, \lstinline{pop} can
block because \lstinline{s.pop} can fail permanently on an empty stack
and \lstinline{swap(elimPop)} can fail permanently if there are no
offers from pushers.

When \lstinline{push} or \lstinline{pop} retry, they follow an
exponential backoff pattern with \emph{spinwaiting} for elimination
offers, exactly as described in \secref{background}.  The backoff
logic follows directly from the implementation of reagents; see
\secref{implementation} for details.

\elide{
On each retry, \lstinline{push} will \emph{spinwait} for
another thread to accept its message along \lstinline{elimPush}; the
length of the wait grows exponentially, as part of the exponential
backoff logic.  Once the waiting time is up, the communication attempt
is cancelled, and the whole reagent is retried.  This protocol is
elaborated in \secref{implementation}.
}

\elide{
This example demonstrates

although we have specified \lstinline{TreiberStack} here

choice on messages, choice with timeout, choice with mix of reagents

emphasize: combine without knowledge of implementation

isolation and interaction
}

\subsection{Composing reagents: sequencing and pairing}
\label{sec:conjunction}

Choice offers a kind of disjunction on reagents.  There are also two
ways of \emph{conjoining} two reagents, so that the composed reagent
has the effect of both underlying reagents:
\begin{itemize} %TODO: graphical depiction?
\item End-to-end composition, via \emph{sequencing}: if 
  \lstinline{r: Reagent[A,B]} and \lstinline{s: Reagent[B,C]} then 
  \lstinline{r >> s: Reagent[A,C]}.
\item Side-by-side composition, via \emph{pairing}: if 
  \lstinline{r: Reagent[A,B]} and \lstinline{s: Reagent[B,C]} then 
  \lstinline{r ||| s: Reagent[A,(B,C)]}.  
\end{itemize}
These combinators differ only in information flow.  Each guarantees
that the atomic actions of both underlying reagents become a single
atomic action for the composition.  For example, if \lstinline{s1} and
\lstinline{s2} are both stacks, then \lstinline{s1.pop >> s2.push} is
a reagent that will atomically transfer an element from the top of one
to the top of the other.  The reagent will block if \lstinline{s1} is
empty.  Similarly, \lstinline{s1.pop ||| s2.pop} will pop, in one
atomic action, the top elements of both stacks, or block if either is
empty.

Here we begin to see the benefits of the reagent abstraction.  Both of
the example combinations work regardless of how the underlying stacks
are implemented.  If both stacks use elimination backoff, the
conjoined operations will potentially use elimination on both
simultaneously.  This behavior is entirely emergent; it does not
require any code on the part of the stack author, and it does not
require the stack user to know anything about the implementation.
Reagents can be composed in unanticpated ways.

The double-pop reagent implicitly gives a solution to the Dining
Philosophers problem~\cite{?}: two resources are atomically acquired.
Internally, the reagent library ensures dead- and live-lock freedom
for conjoined reagents.

The implementation details for conjunctions are discussed
later~(\secref{implementation}), but a key point is that the performance cost
is \emph{pay as you go}.  Single atomic reagents like \lstinline{push} and
\lstinline{pop} execute a single CAS---just like the standard nonblocking
algorithms they are meant to implement---even though these operations can be
combined into larger atomic operations.  The cost of conjunction is only
incurred when a conjoined reagent is actually used.  This is a crucial
difference from STM, which generally incurs overheads regardless of the size
of the atomic blocks; see~\secref{related} for more discussion.

\elide{
Unlike function composition, however, the
atomicity and blocking behavior of the reagents is combined.

fills a key gap in current libraries for fine-grained concurrency.

KEY: pay as you go!

dining philosophers

pop from stack, send as message

exponential search for message passing

show how, for counter, tryDec and dec are inter-expressible
}

\subsection{Catalysts: persistent reagents}
\label{sec:catalysts}

In chemistry, a reagent is a participant in a reaction, and reagents are
further subdivided into \emph{reactants}, which are consumed during reaction,
and \emph{catalysts}, which enable reactions but are not consumed by them.

The \lstinline{!} operator invokes a reagent as a reactant: the invocation
lasts for a single reaction, and any messages sent by the reagent are
consumed.  But sometimes it is useful for reagents to persist beyond a single
reaction, \emph{i.e.}, to act as catalysts.  For example, the following
function creates a catalyst that merges input from two endpoints and sends the
resulting pairs to another endpoint:
\begin{lstlisting}
def zip(in1: Endpoint[Unit,  A], 
        in2: Endpoint[Unit,  B], 
        out: Endpoint[(A,B), Unit]) = 
  dissolve((swap(in1) ||| swap(in2)) >> swap(out))
\end{lstlisting}
The \lstinline{dissolve} function takes a \lstinline{Reagent[Unit, Unit]} and
introduces it as a catalyst.\footnote{For simplicity, we have not given a way
  to \emph{cancel} catalysts after they have been introduced, but cancellation
  is easy to add.}  Operationally, in this example, that just means sending
messages along \lstinline{in1} and \listinline{in2} that are marked as
catalyzing messages, and hence are not consumed during reaction.  The upshot
is that senders along \lstinline{in1} will see the catalyzing messages, look
for messages along \lstinline{in2} to pair with, and ultimately send messages
along \lstinline{out} (and similarly in the other order).

Catalysts could instead be expressed using a thread that repeatedly invokes a
reagent as a reactant.  Allowing direct expression through
\lstinline{dissolve} is more efficient (since it does not tie up a thread) and
allows greater parallelism (since, as with the \lstinline{zip} example above,
multiple reagents can react with it in parallel).

Catalysts are not limited to message passing.  The \lstinline{zip} example
above could be rephrased to take reagents, rather than endpoints, as
arguments.  

%TODO: barrier?
%TODO: explain Unit -> Unit?

% nice catalysts: zip and unzip, tree barrier

\elide{
class TreeBarrier {
  public readonly Synchronous.Channel[] Arrive;
  private readonly Join j; // create j, init chans ...
  public TreeBarrier(int n) {Wire(0, n-1, () => {});}
  private void Wire(int low, int high, Action Done) {
    if (low == high) j.When(Arrive[low]).Do(Done);
    else if (low + 1 == high)
      j.When(Arrive[low]).And(Arrive[high]).Do(Done);
    else { // low + 1 < high
      Synchronous.Channel Left, Right; // init chans       
      j.When(Left).And(Right).Do(Done);
      int mid = (low + high) / 2;
      Wire(low, mid, () => Left());
      Wire(mid + 1, high, () => Right()); 
    }
  }
}
}
\subsection{Dynamic reagents and \lstinline{cas}}

The combinators introduced so far are powerful, but they impose a strict phase
separation: reagents are constructed prior to, and independently from, the
data that flows through them.  Phase separation is useful, because it allows
reagent execution to be optimized based on complete knowledge of the
computation to be performed (see \secref{implementation}).  But in many cases
the structure of the computation to be performed depends on the input or other
dynamic data.

\begin{figure}
\begin{lstlisting}
class MSQueue[A] {
  private case class Node(data: A, next: Ref[Node])
  private val head = new Ref(Node(null))
  private val tail = new Ref(head.read ! ())
  val tryDeq: Reagent[Unit, Option[A]] = upd(head) {
    case (Node(_, Ref(n@Node(x, _))), ()) => (n, Some(x))
    case (emp,                        ()) => (emp, None)
  }
  private def findAndEnq(n: Node): Reagent[Unit,Unit] = 
    read(tail) ! () match {
      case ov@Node(_, r@Ref(null)) => 
        cas(r, null, n) >>
        postCommit { _ => cas(tail, ov, n) !? () }
      case ov@Node(_, Ref(nv)) => 
        cas(tail, ov, nv) !? (); 
        findAndEnq(n)
    }
  val enq: Reagent[A, Unit] = computed { 
    (x: A) => findAndEnq(Node(x, new Ref(null)))
  }
}
\end{lstlisting}
\nocaptionrule
\caption{The Michael-Scott queue, with reagents}
\label{fig:msqueue}
\end{figure}

An example of this phenomenon is the classic Michael-Scott lock-free
queue~\cite{?}. 

\elide{
The \lstinline{read} combinator has a straightforward signature: if
\lstinline{r} has type \lstinline{Ref[A]}, then \lstinline{read(r)} is a
\emph{value} of type \lstinline{Reagent[Unit, A]} which, when invoked, takes a
unit\footnote{\lstinline{Unit} in Scala is akin to \lstinline{void}: it is a
  type with a single member, written \lstinline{()}.} argument and yields an
\lstinline{A} result.

The \lstinline{cas} combinator takes two additional \lstinline{A} arguments,
giving the expected and updated values, respectively.  Unlike its counterpart
for \lstinline{AtomicReference}, a \lstinline{cas} reagent does \emph{not}
yield a boolean result.  Instead, the reagent library builds in handling of
failed CASes, including an optimistic retry loop and exponential backoff.  The
failure mechanics depend on which other combinators are involved, so we delay
a complete discussion until \secref{failure}.
}
 
% tail recursive

This is where the differences from STM really start to show.

post-commit action for catching up tail

\paragraph{A caveat for composition}

\section{Implementation}
\label{sec:implementation}

bags allow greatest degree of freedom and parallelism

kcas implementations: \cite{Attiya2008,Fraser2007,Luchangco2003}

\begin{lstlisting}
def !(a: A): B = tryReact(a, Reaction.inert, null) match {
  case (_: BacktrackCommand) => {
    val backoff = new Backoff
    val maySync = this.maySync // cache
    @tailrec def retryLoop(shouldBlock: Boolean): B = {
      val wait = maySync || shouldBlock
      val waiter = if (wait) new Waiter[B](shouldBlock) 
                   else null

      tryReact(a, Reaction.inert, waiter) match {
        case (bc: BacktrackCommand) if wait => {
          bc.bottom(waiter, backoff, snoop(a))
          waiter.tryAbort      // rescind waiter,
          waiter.poll match { // but check if completed
            case Some(ans) => ans.asInstanceOf[B] 
            case None       => retryLoop(bc.isBlock)
          }
        }
        case Retry => backoff.once; retryLoop(false)
        case Block => retryLoop(true)
        case ans   => ans.asInstanceOf[B]
      }
    }
    retryLoop(false)
  }
  case ans => ans.asInstanceOf[B]
}
\end{lstlisting}

\begin{lstlisting}
def tryReact(a: A, rx: Reaction, offer: Offer[B]): Any = 
  r1.tryReact(a, rx, offer) match {
    case Retry => 
      r2.tryReact(a, rx, offer) match {
        case Retry => Retry
        case Block => Retry // must retry r1
        case ans   => ans
      }
    case Block => r2.tryReact(a, rx, offer)
    case ans => ans
  }
\end{lstlisting}

\begin{lstlisting}
  def tryReact(u: Unit, rx: Reaction, offer: Offer[B]): Any = 
    if (rx.canCASImmediate(k, offer)) {
      if (casI(expect, update))
        k.tryReact((), rx, offer)
      else Retry
    } else k.tryReact((), rx.withCAS(Ref.this, expect, update), offer)
\end{lstlisting}

\section{Performance}
\label{sec:performance}



\section{Related work}
\label{sec:related}

The design of reagents builds on a foundation laid by Concurrent ML
and extended in Haskell's STM

\subsection{Concurrent ML}

pure cml~\cite{Reppy1991}

parallel CML's implementation of choice~\cite{Reppy2009}

\subsection{Software transactional memory}

STM in general

Haskell's STM~\cite{Harris2005a}

left-biased choice, but differently from \lstinline{orElse} in
Haskell's STM.

\subsection{Transactions that communicate}

Communicating transactions~\cite{Lesani2011}, 
Transaction synchronizers~\cite{Luchangco2005},
Transaction communicators~\cite{Luchangco2011},
transactional events~\cite{Donnelly2006}

\subsection{Composing lock-free data structures}

elastic transactions~\cite{Felber2009} and company, 

Paper on composing lock-free data structure operations~\cite{Cederman2010}

\section{Conclusion}

We believe it is crucial to have XX, YY together


\bibliographystyle{abbrvnat}
\bibliography{reagents}

\end{document}

# Intro notes

What do we want to accomplish?

 - motivate libraries like j.u.c
 - downsides of such libraries
   - huge research & impl effort
   - not easily extensible
   - internal structure has repeated, but unabstracted patterns
 - our contribution
   - a new way of expressing scalable algorithms that
     - doesn't lose much performance 
     - allows composition
     - is much nicer than direct programming
     - also incorporates blocking behavior

# General notes

Imposes no cache-coherence overhead on isolated data structures

Can argue that transactional-event-style liveness properties are
important if you want general way of composing fine-grained concurrent
algorithms with message-passing

To what extent do reagents encompass STM?  who are we competing with?

multiparadim, expressive, fast -- all that's great, but sales pitch
needs to be oriented around a customer with specific needs.  any
reason not to use the pitch of extensible, composable j.u.c.?

# To do

  - benchmark against STM (possibly Akka Transactors?)

  - benchmark queues, including flat combining and buckets

  - somewhere say that: we examine fairly simple examples in the
  paper, due to size constraints.  but we can handle more complex
  cases like sets/maps represented via skip lists, basket queues, 
  and are exploring flat combining.

# Outline

1.2 Title, abstract, intro,             1.0
1.0 Background                          1.0
2.5 Library overview & examples         3.0
1.5 Semantics 
1.5 Implementation/algorithms           2.0
0.5 Performance results                 1.0
1.0 Related work                        1.0
0.3 Conclusion                          0.2
0.8 Bib                                 0.8

How much time should we spend covering scalable concurrency
background?  This is perhaps tied to strategies for introducing the
library constructs: the primitives can be shown by-need for examples.
Should consult good papers like Ryan's and Jesse's for examples here.

# Semantics

Need to nail down story for overlapping state access.  In particular,
when (if ever) is an error flagged?  Unfortunately, talking about heap
unsplittability is rather more complicated than splittability.

Examples:

\begin{itemize}
\item TreiberStack
\item with blocking
\item with elimination backoff
\item MSQueue
\item with blocking
\item with buckets?
\item Counter to semaphore
\item RWLocks
\item Tree barriers
\item Dining philosophers(ish)
\item Flat combining - doesn't work if commit is two-phase process.
  probably too much to bring into scope for this paper, anyway
\end{itemize}


\elide{
\section{Semantics}
\label{sec:semantics}

Try to keep this minimal for the sake of space

Purpose of the semantics is mainly to clarify the relationship between
the shared-state and message-passing aspects of reagents.  We have
ruthlessly stripped out ...

(fractional?) permissions model

Left bias in choice operator

\begin{itemize}
  \item read, cas
  \item send
  \item choice
  \item react
  \item bind and ret
  \item computed?
\end{itemize}

Computed and lift cause problems: can potentially invoke a reagent
inside another invocation.  Just drop these.  And probably ret too.
Hmm.  This doesn't even allow us to express upd.  But upd itself is
problematic for the same reasons.

Could easily split out ``pure'' expressions.  Unsatisfying...  but
this is essentially what Haskell's STM is doing.

% NOTE: this leaves a nice opportunity later to give a full semantic
% story

\elide{
\[
\infer
  {P_i, \sigma_i \cstep{t_i} P'_i, \sigma'_i}
  {P_1 | \cdots | P_n,\ \sigma_1 \uplus \cdots \uplus \sigma_n\ \step\
   P'_1 | \cdots | P'_n,\ \sigma'_1 \uplus \cdots \uplus \sigma'_n
  }
\]

\[
\infer
  {r_i!v_i, \sigma_i \cstep{t_i} \ret{v'_i}, \sigma'_i}
  {\procCtx[\prod \evalCtx_i[r_i ! v_i]],\ \biguplus \sigma_i \step
   \procCtx[\prod \evalCtx_i[\ret{v'_i}]],\ \biguplus \sigma'_i}
\]
}

\[
\infer
  {r_i\ \lstinline{!}\ v_i, \sigma_i \cstep{t_i} \ret{v'_i}, \sigma'_i}
  {\prod \evalCtx_i[r_i ! v_i],\ \biguplus \sigma_i \step
   \prod \evalCtx_i[\ret{v'_i}],\ \biguplus \sigma'_i}
\]
}
