\documentclass[preprint]{sigplanconf}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathpartir}

\newcommand{\ra}{\rightarrow}
\newcommand{\gives}{\vdash}
\newcommand{\proves}{\vdash}
\newcommand{\GA}{\ |\ }
\newcommand{\kw}[1]{\textsf{#1}}
\newcommand{\ty}[1]{\textrm{#1}\ }
\newcommand{\bind}{\mbox{\ \scriptsize$>\!\!>\!=$\ }}
\newcommand{\letrec}[2]{\kw{letrec}\ #1\ \kw{in}\ #2}
\newcommand{\LET}[4]{\kw{let}_{#1}\ #2 = #3\ \kw{in}\ #4}

\begin{document}

\authorinfo{}{}{}
% yikes!
\title{Reagents: expressing and composing patterns of fine-grained concurrency}
\maketitle

\begin{abstract}
blah blah
\end{abstract}

\section{Introduction}

\hspace{\stretch{1}}\emph{Programs are what happens between cache
  misses}.\footnote{Folklore; origin unknown.}

\subsection*{The problem}

Ahmdahl's law tells us that sequential bottlenecks fundamentally limit
profit from parallelism~\cite{?}.  In practice, the effect is made
fatal by a silent killer: interprocessor communication, often in the
form of cache coherence.  When one thread waits on another, the
program pays the cost of lost parallelism \emph{and} an extra cache
miss.  Extra misses can easily add up to parallel \emph{slowdown},
more than negating the benefits of the remaining parallelism.  Cache,
as ever, is king.

The easy answer is: avoid communication.  In other words, parallelize
at a coarse grain, giving threads large chunks of independent work.
That strategy only goes so far.  Some work doesn't easily factor into
large chunks, or equal size chunks.  Fine-grained parallelism is
easier to find, and easier to sprinkle throughout existing sequential
code.  With increasing pressure to parallelize, fine-grained
parallelism will only become more common.

In the past two decades,\footnote{Herlihy/Wing, Mellor-Crummey/Scott}
an enormous literature has emerged 

\subsection*{Our contribution}

\section{Reagents}

\section{Semantics}

\section{Implemention}

\section{Performance}

\section{Related work}

\bibliographystyle{abbrvnat}

\end{document}

# Intro notes

What do we want to accomplish?

 - motivate libraries like j.u.c
 - downsides of such libraries
   - huge research & impl effort
   - not easily extensible
   - internal structure has repeated, but unabstracted patterns
 - our contribution
   - a new way of expressing scalable algorithms that
     - doesn't lose much performance 
     - allows composition
     - is much nicer than direct programming
     - also incorporates blocking behavior

# General notes

Imposes no cache-coherence overhead on isolated data structures

Can argue that transactional-event-style liveness properties are
important if you want general way of composing fine-grained concurrent
algorithms with message-passing

To what extent do reagents encompass STM?  who are we competing with?

multiparadim, expressive, fast -- all that's great, but sales pitch
needs to be oriented around a customer with specific needs.  any
reason not to use the pitch of extensible, composable j.u.c.?

# Contributions

  - make it easier to express fine-grained concurrent algorithms,
  including blocking operations etc.  make it possible to build them
  compositionally, which already happens in the literature but is
  largely unremarked upon.

  - we capture exponential backoff, elimination backoff, and other
  such patterns once and for all

  - can use flat combining as illustration of capturing very general
  concurrency abstraction

  - smooth blend of message-passing and shared-state concurrency
  isolation for shared state, coherent communication through messages,
  uniform blocking/synchronization

  - operational semantics

  - yields completely lock-free implementation of core CML

  - generalizes join calculus implementation, allowing dissolution
  even for existing channels

  - generalizes transactions events, but with much better
  implementation

# To do

  - benchmark against STM (possibly Akka Transactors?)

  - benchmark queues, including flat combining and buckets

# Outline

2.5 Title, abstract, intro, library overview 
1.5 Semantics 
2.5 Implementation/algorithms 
1.0 Performance results
1.5 Related work
1.0 Conclusion and bib

How much time should we spend covering scalable concurrency
background?  This is perhaps tied to strategies for introducing the
library constructs: the primitives can be shown by-need for examples.
Should consult good papers like Ryan's and Jesse's for examples here.

# Semantics

Need to nail down story for overlapping state access.  In particular,
when (if ever) is an error flagged?  Unfortunately, talking about heap
unsplittability is rather more complicated than splittability.
